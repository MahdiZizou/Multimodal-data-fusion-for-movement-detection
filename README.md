# Multimodal-data-fusion-for-movement-detection
Multi-Modal Physical Exercise Classification
The goal of this project is to develop user-independent pre-processing and classification models to recognize 7 different physical exercises measured by accelerometer (attached to subject's thigh) and depth camera (above the subject facing downwards recording an aerial view). All the exercises were performed subject lying down on the mat. Original dataset have also another acceleration sensor and pressure-sensitive mat, but those two modalities are ommited in this project. There are totally 30 subjects in the original dataset, and in this work subset of 10 person is utilized. Detailed description of the dataset and original data can be access in [MEx dataset @ UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/MEx#). We are providing the subset of dataset in Moodle.
The project work is divided on following phases:
- Data preparation, exploration, and visualization
- Feature extraction and unimodal fusion for classification
- Feature extraction and feature-level fusion for multimodal classification
- Decision-level fusion for multimodal classification
